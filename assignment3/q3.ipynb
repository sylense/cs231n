{
 "nbformat": 3, 
 "nbformat_minor": 0, 
 "worksheets": [
  {
   "cells": [
    {
     "source": [
      "# Transfer learning\n", 
      "In the previous exercise we introduced the TinyImageNet-100-A dataset, and combined a handful of pretrained models on this dataset to improve our classification performance.\n", 
      "\n", 
      "In this exercise we will explore several ways to adapt one of these same pretrained models to the TinyImageNet-100-B dataset, which does not share any images or object classes with TinyImage-100-A. We will see that we can use a pretrained classfier together with a small amount of training data from TinyImageNet-100-B to achieve reasonable performance on the TinyImageNet-100-B validation set."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "# A bit of setup\n", 
      "\n", 
      "import numpy as np\n", 
      "import matplotlib.pyplot as plt\n", 
      "from time import time\n", 
      "\n", 
      "%matplotlib inline\n", 
      "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n", 
      "plt.rcParams['image.interpolation'] = 'nearest'\n", 
      "plt.rcParams['image.cmap'] = 'gray'\n", 
      "\n", 
      "# for auto-reloading extenrnal modules\n", 
      "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n", 
      "%load_ext autoreload\n", 
      "%autoreload 2"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Load data and model\n", 
      "You should already have downloaded the TinyImageNet-100-A and TinyImageNet-100-B datasets along with the pretrained models. Run the cell below to load (a subset of) the TinyImageNet-100-B dataset and one of the models that was pretrained on TinyImageNet-100-A.\n", 
      "\n", 
      "TinyImageNet-100-B contains 50,000 training images in total (500 per class for all 100 classes) but for this exercise we will use only 5,000 training images (50 per class on average)."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "# Load the TinyImageNet-100-B dataset\n", 
      "\n", 
      "from cs231n.data_utils import load_tiny_imagenet, load_models\n", 
      "\n", 
      "tiny_imagenet_b = 'cs231n/datasets/tiny-imagenet-100-B'\n", 
      "        \n", 
      "class_names, X_train, y_train, X_val, y_val, X_test, y_test = load_tiny_imagenet(tiny_imagenet_b)\n", 
      "\n", 
      "# Zero-mean the data\n", 
      "mean_img = np.mean(X_train, axis=0)\n", 
      "X_train -= mean_img\n", 
      "X_val -= mean_img\n", 
      "X_test -= mean_img\n", 
      "\n", 
      "# We will use a subset of the TinyImageNet-B training data\n", 
      "mask = np.random.choice(X_train.shape[0], size=5000, replace=False)\n", 
      "X_train = X_train[mask]\n", 
      "y_train = y_train[mask]\n", 
      "\n", 
      "# Load a pretrained model; it is a five layer convnet.\n", 
      "models_dir = 'cs231n/datasets/tiny-100-A-pretrained'\n", 
      "model = load_models(models_dir)['model1']"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# TinyImageNet-100-B classes\n", 
      "In the previous assignment we printed out a list of all classes in TinyImageNet-100-A. We can do the same on TinyImageNet-100-B; if you compare with the list in the previous exercise you will see that there is no overlap between the classes in TinyImageNet-100-A and TinyImageNet-100-B."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "for names in class_names:\n", 
      "    print ' '.join('\"%s\"' % name for name in names)"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Visualize Examples\n", 
      "Similar to the previous exercise, we can visualize examples from the TinyImageNet-100-B dataset. The images are similar to TinyImageNet-100-A, but the images and classes in the two datasets are disjoint."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "# Visualize some examples of the training data\n", 
      "classes_to_show = 7\n", 
      "examples_per_class = 5\n", 
      "\n", 
      "class_idxs = np.random.choice(len(class_names), size=classes_to_show, replace=False)\n", 
      "for i, class_idx in enumerate(class_idxs):\n", 
      "    train_idxs, = np.nonzero(y_train == class_idx)\n", 
      "    train_idxs = np.random.choice(train_idxs, size=examples_per_class, replace=False)\n", 
      "    for j, train_idx in enumerate(train_idxs):\n", 
      "        img = X_train[train_idx] + mean_img\n", 
      "        img = img.transpose(1, 2, 0).astype('uint8')\n", 
      "        plt.subplot(examples_per_class, classes_to_show, 1 + i + classes_to_show * j)\n", 
      "        if j == 0:\n", 
      "            plt.title(class_names[class_idx][0])\n", 
      "        plt.imshow(img)\n", 
      "        plt.gca().axis('off')\n", 
      "\n", 
      "plt.show()"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Extract features\n", 
      "ConvNets tend to learn generalizable high-level image features. For the five layer ConvNet architecture, we will use the (rectified) activations of the first fully-connected layer as our high-level image features.\n", 
      "\n", 
      "Open the file `cs231n/classifiers/convnet.py` and modify the `five_layer_convnet` function to return features when the `extract_features` flag is `True`. This should be VERY simple.\n", 
      "\n", 
      "Once you have done that, fill in the cell below, which should use the pretrained model in the `model` variable to extract features from all images in the training and validation sets."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "from cs231n.classifiers.convnet import five_layer_convnet\n", 
      "\n", 
      "# These should store extracted features for the training and validation sets\n", 
      "# respectively.\n", 
      "#\n", 
      "# More concretely, X_train_feats should be an array of shape\n", 
      "# (X_train.shape[0], 512) where X_train_feats[i] is the 512-dimensional\n", 
      "# feature vector extracted from X_train[i] using model.\n", 
      "#\n", 
      "# Similarly X_val_feats should have shape (X_val.shape[0], 512) and\n", 
      "# X_val_feats[i] should be the 512-dimensional feature vector extracted from\n", 
      "# X_val[i] using model.\n", 
      "X_train_feats = None\n", 
      "X_val_feats = None\n", 
      "\n", 
      "# Use our pre-trained model to extract features on the subsampled training set\n", 
      "# and the validation set.\n", 
      "\n", 
      "################################################################################\n", 
      "# TODO: Use the pretrained model to extract features for the training and      #\n", 
      "# validation sets for TinyImageNet-100-B.                                      #\n", 
      "#                                                                              #\n", 
      "# HINT: Similar to computing probabilities in the previous exercise, you       #\n", 
      "# should split the training and validation sets into small batches to avoid    #\n", 
      "# using absurd amounts of memory.                                              #\n", 
      "################################################################################\n", 
      "pass\n", 
      "################################################################################\n", 
      "#                            END OF YOUR CODE                                  #\n", 
      "################################################################################   "
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# kNN with ConvNet features\n", 
      "A simple way to implement transfer learning is to use a k-nearest neighborhood classifier. However instead of computing the distance between images using their pixel values as we did in Assignment 1, we will instead say that the distance between a pair of images is equal to the L2 distance between their feature vectors extracted using our pretrained ConvNet.\n", 
      "\n", 
      "Implement this idea in the cell below. You can use the `KNearestNeighbor` class in the file `cs321n/classifiers/k_nearest_neighbor.py`."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "from cs231n.classifiers.k_nearest_neighbor import KNearestNeighbor\n", 
      "\n", 
      "# Predicted labels for X_val using a k-nearest-neighbor classifier trained on\n", 
      "# the features extracted from X_train. knn_y_val_pred[i] = c indicates that\n", 
      "# the kNN classifier predicts that X_val[i] has label c.\n", 
      "knn_y_val_pred = None\n", 
      "\n", 
      "################################################################################\n", 
      "# TODO: Use a k-nearest neighbor classifier to compute knn_y_val_pred.         #\n", 
      "# You may need to experiment with k to get the best performance.               #\n", 
      "################################################################################\n", 
      "pass\n", 
      "################################################################################\n", 
      "#                            END OF YOUR CODE                                  #\n", 
      "################################################################################\n", 
      "\n", 
      "print 'Validation set accuracy: %f' % np.mean(knn_y_val_pred == y_val)"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Visualize neighbors\n", 
      "Recall that the kNN classifier computes the distance between all of its training instances and all of its test instances. We can use this distance matrix to help understand what the ConvNet features care about; specifically, we can select several random images from the validation set and visualize their nearest neighbors in the training set.\n", 
      "\n", 
      "You will see that many times the nearest neighbors are quite far away from each other in pixel space; for example two images that show the same object from different perspectives may appear nearby in ConvNet feature space.\n", 
      "\n", 
      "Since the following cell selects random validation images, you can run it several times to get different results."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "dists = knn.compute_distances_no_loops(X_val_feats)\n", 
      "\n", 
      "num_imgs = 5\n", 
      "neighbors_to_show = 6\n", 
      "\n", 
      "query_idxs = np.random.randint(X_val.shape[0], size=num_imgs)\n", 
      "\n", 
      "next_subplot = 1\n", 
      "first_row = True\n", 
      "for query_idx in query_idxs:\n", 
      "    query_img = X_val[query_idx] + mean_img\n", 
      "    query_img = query_img.transpose(1, 2, 0).astype('uint8')\n", 
      "    plt.subplot(num_imgs, neighbors_to_show + 1, next_subplot)\n", 
      "    plt.imshow(query_img)\n", 
      "    plt.gca().axis('off')\n", 
      "    if first_row:\n", 
      "        plt.title('query')\n", 
      "    next_subplot += 1\n", 
      "    o = np.argsort(dists[query_idx])\n", 
      "    for i in xrange(neighbors_to_show):            \n", 
      "        img = X_train[o[i]] + mean_img\n", 
      "        img = img.transpose(1, 2, 0).astype('uint8')\n", 
      "        plt.subplot(num_imgs, neighbors_to_show + 1, next_subplot)\n", 
      "        plt.imshow(img)\n", 
      "        plt.gca().axis('off')\n", 
      "        if first_row:\n", 
      "            plt.title('neighbor %d' % (i + 1))\n", 
      "        next_subplot += 1\n", 
      "    first_row = False\n", 
      "    "
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Softmax on ConvNet features\n", 
      "Another way to implement transfer learning is to train a linear classifier on top of the features extracted from our pretrained ConvNet.\n", 
      "\n", 
      "In the cell below, train a softmax classifier on the features extracted from the training set of TinyImageNet-100-B and use this classifier to predict on the validation set for TinyImageNet-100-B. You can use the `Softmax` class in the file `cs231n/classifiers/linear_classifier.py`."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "from cs231n.classifiers.linear_classifier import Softmax\n", 
      "\n", 
      "softmax_y_train_pred = None\n", 
      "softmax_y_val_pred = None\n", 
      "\n", 
      "################################################################################\n", 
      "# TODO: Train a softmax classifier to predict a TinyImageNet-100-B class from  #\n", 
      "# features extracted from our pretrained ConvNet. Use this classifier to make  #\n", 
      "# predictions for the TinyImageNet-100-B training and validation sets, and     #\n", 
      "# store them in softmax_y_train_pred and softmax_y_val_pred.                   #\n", 
      "#                                                                              #\n", 
      "# You may need to experiment with number of iterations, regularization, and    #\n", 
      "# learning rate in order to get good performance. The softmax classifier       #\n", 
      "# should achieve a higher validation accuracy than the kNN classifier.         #\n", 
      "################################################################################\n", 
      "pass\n", 
      "################################################################################\n", 
      "#                            END OF YOUR CODE                                  #\n", 
      "################################################################################\n", 
      "\n", 
      "train_acc = np.mean(y_train == y_train_pred)\n", 
      "val_acc = np.mean(y_val_pred == y_val)\n", 
      "print train_acc, val_acc"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Fine-tuning\n", 
      "We can improve our classification results on TinyImageNet-100-B further by fine-tuning our ConvNet. In other words, we will train a new ConvNet with the same architecture as our pretrained model, and use the weights of the pretrained model as an initialization to our new model.\n", 
      "\n", 
      "Usually when fine-tuning you would re-initialize the weights of the final affine layer randomly, but in this case we will initialize the weights of the final affine layer using the weights of the trained softmax classifier from above.\n", 
      "\n", 
      "In the cell below, use fine-tuning to improve your classification performance on TinyImageNet-100-B. You should be able to outperform the softmax classifier from above using fewer than 5 epochs over the training data.\n", 
      "\n", 
      "You will need to adjust the learning rate and regularization to achieve good fine-tuning results."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "from cs231n.classifier_trainer import ClassifierTrainer\n", 
      "\n", 
      "# Make a copy of the pretrained model\n", 
      "model_copy = {k: v.copy() for k, v in model.iteritems()}\n", 
      "\n", 
      "# Initialize the weights of the last affine layer using the trained weights from\n", 
      "# the softmax classifier above\n", 
      "model_copy['W5'] = softmax.W.T.copy().astype(model_copy['W5'].dtype)\n", 
      "model_copy['b5'] = np.zeros_like(model_copy['b5'])\n", 
      "\n", 
      "# Fine-tune the model. You will need to adjust the training parameters to get good results.\n", 
      "trainer = ClassifierTrainer()\n", 
      "learning_rate = 1e-3\n", 
      "reg = 1e-2\n", 
      "dropout = 0.5\n", 
      "num_epochs = 1\n", 
      "finetuned_model = trainer.train(X_train, y_train, X_val, y_val,\n", 
      "                                model_copy, five_layer_convnet,\n", 
      "                                learning_rate=learning_rate, reg=reg, update='rmsprop',\n", 
      "                                dropout=dropout, num_epochs=num_epochs, verbose=True)[0]"
     ], 
     "metadata": {}
    }
   ], 
   "metadata": {}
  }
 ], 
 "metadata": {
  "name": "", 
  "signature": "sha256:179db24504892a77af15768175172b131478f173609a9f9340cd0851bd529dbc"
 }
}