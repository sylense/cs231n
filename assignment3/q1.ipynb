{
 "nbformat": 3, 
 "nbformat_minor": 0, 
 "worksheets": [
  {
   "cells": [
    {
     "source": [
      "# Dropout and Data Augmentation\n", 
      "In this exercise we will implement two ways to reduce overfitting.\n", 
      "\n", 
      "Like the previous assignment, we will train ConvNets to recognize the categories in CIFAR-10. However unlike the previous assignment where we used 49,000 images for training, in this exercise we will use just 500 images for training.\n", 
      "\n", 
      "If we try to train a high-capacity model like a ConvNet on this small amount of data, we expect to overfit, and end up with a solution that does not generalize. We will see that we can drastically reduce overfitting by using dropout and data augmentation."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "# A bit of setup\n", 
      "\n", 
      "import numpy as np\n", 
      "import matplotlib.pyplot as plt\n", 
      "from time import time\n", 
      "from cs231n.layers import *\n", 
      "from cs231n.fast_layers import *\n", 
      "\n", 
      "%matplotlib inline\n", 
      "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n", 
      "plt.rcParams['image.interpolation'] = 'nearest'\n", 
      "plt.rcParams['image.cmap'] = 'gray'\n", 
      "\n", 
      "# for auto-reloading extenrnal modules\n", 
      "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n", 
      "%load_ext autoreload\n", 
      "%autoreload 2\n", 
      "\n", 
      "def rel_error(x, y):\n", 
      "    \"\"\" returns relative error \"\"\"\n", 
      "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Load data\n", 
      "For this exercise our training set will contain 500 images and our validation and test sets will contain 1000 images as usual."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "from cs231n.data_utils import load_CIFAR10\n", 
      "\n", 
      "def get_CIFAR10_data(num_training=500, num_validation=1000, num_test=1000, normalize=True):\n", 
      "    \"\"\"\n", 
      "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n", 
      "    it for the two-layer neural net classifier. These are the same steps as\n", 
      "    we used for the SVM, but condensed to a single function.  \n", 
      "    \"\"\"\n", 
      "    # Load the raw CIFAR-10 data\n", 
      "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n", 
      "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n", 
      "        \n", 
      "    # Subsample the data\n", 
      "    mask = range(num_training, num_training + num_validation)\n", 
      "    X_val = X_train[mask]\n", 
      "    y_val = y_train[mask]\n", 
      "    mask = range(num_training)\n", 
      "    X_train = X_train[mask]\n", 
      "    y_train = y_train[mask]\n", 
      "    mask = range(num_test)\n", 
      "    X_test = X_test[mask]\n", 
      "    y_test = y_test[mask]\n", 
      "\n", 
      "    # Normalize the data: subtract the mean image\n", 
      "    if normalize:\n", 
      "        mean_image = np.mean(X_train, axis=0)\n", 
      "        X_train -= mean_image\n", 
      "        X_val -= mean_image\n", 
      "        X_test -= mean_image\n", 
      "    \n", 
      "    # Transpose so that channels come first\n", 
      "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n", 
      "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n", 
      "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n", 
      "\n", 
      "    return X_train, y_train, X_val, y_val, X_test, y_test\n", 
      "\n", 
      "\n", 
      "# Invoke the above function to get our data.\n", 
      "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(num_training=500)\n", 
      "print 'Train data shape: ', X_train.shape\n", 
      "print 'Train labels shape: ', y_train.shape\n", 
      "print 'Validation data shape: ', X_val.shape\n", 
      "print 'Validation labels shape: ', y_val.shape\n", 
      "print 'Test data shape: ', X_test.shape\n", 
      "print 'Test labels shape: ', y_test.shape"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Overfit\n", 
      "Now that we've loaded our data, we will attempt to train a three layer convnet on this data. The three layer convnet has the architecture\n", 
      "\n", 
      "`conv - relu - pool - affine - relu - affine - softmax`\n", 
      "\n", 
      "We will use 32 5x5 filters, and our hidden affine layer will have 128 neurons.\n", 
      "\n", 
      "This is a very expressive model given that we have only 500 training samples, so we should expect to massively overfit this dataset, and achieve a training accuracy of nearly 0.9 with a much lower validation accuracy."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "from cs231n.classifiers.convnet import *\n", 
      "from cs231n.classifier_trainer import ClassifierTrainer\n", 
      "\n", 
      "model = init_three_layer_convnet(filter_size=5, num_filters=(32, 128))\n", 
      "trainer = ClassifierTrainer()\n", 
      "best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n", 
      "          X_train, y_train, X_val, y_val, model, three_layer_convnet, dropout=None,\n", 
      "          reg=0.05, learning_rate=0.00005, batch_size=50, num_epochs=15,\n", 
      "          learning_rate_decay=1.0, update='rmsprop', verbose=True)"
     ], 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "# Visualize the loss and accuracy for our network trained on a small dataset\n", 
      "\n", 
      "plt.subplot(2, 1, 1)\n", 
      "plt.plot(train_acc_history)\n", 
      "plt.plot(val_acc_history)\n", 
      "plt.title('accuracy vs time')\n", 
      "plt.legend(['train', 'val'], loc=4)\n", 
      "plt.xlabel('epoch')\n", 
      "plt.ylabel('classification accuracy')\n", 
      "\n", 
      "plt.subplot(2, 1, 2)\n", 
      "plt.plot(loss_history)\n", 
      "plt.title('loss vs time')\n", 
      "plt.xlabel('iteration')\n", 
      "plt.ylabel('loss')\n", 
      "plt.show()"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Dropout\n", 
      "The first way we will reduce overfitting is to use dropout.\n", 
      "\n", 
      "Open the file `cs231n/layers.py` and implement the `dropout_forward` and `dropout_backward` functions. We can check the forward pass by looking at the statistics of the outputs in train and test modes, and we can check the backward pass using numerical gradient checking."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "# Check the dropout forward pass\n", 
      "\n", 
      "x = np.random.randn(100, 100)\n", 
      "dropout_param_train = {'p': 0.25, 'mode': 'train'}\n", 
      "dropout_param_test = {'p': 0.25, 'mode': 'test'}\n", 
      "\n", 
      "out_train, _ = dropout_forward(x, dropout_param_train)\n", 
      "out_test, _ = dropout_forward(x, dropout_param_test)\n", 
      "\n", 
      "# Test dropout training mode; about 25% of the elements should be nonzero\n", 
      "print np.mean(out_train != 0)\n", 
      "\n", 
      "# Test dropout test mode; all of the elements should be nonzero\n", 
      "print np.mean(out_test != 0)"
     ], 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "from cs231n.gradient_check import eval_numerical_gradient_array\n", 
      "\n", 
      "# Check the dropout backward pass\n", 
      "\n", 
      "x = np.random.randn(5, 4)\n", 
      "dout = np.random.randn(*x.shape)\n", 
      "dropout_param = {'p': 0.8, 'mode': 'train', 'seed': 123}\n", 
      "\n", 
      "dx_num = eval_numerical_gradient_array(lambda x: dropout_forward(x, dropout_param)[0], x, dout)\n", 
      "\n", 
      "_, cache = dropout_forward(x, dropout_param)\n", 
      "dx = dropout_backward(dout, cache)\n", 
      "\n", 
      "# The error should be around 1e-12\n", 
      "print 'Testing dropout_backward function:'\n", 
      "print 'dx error: ', rel_error(dx_num, dx)"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Data Augmentation\n", 
      "The next way we will reduce overfitting is to implement data augmentation. Since we have very little training data, we will use what little training data we have to generate artificial data, and use this artificial data to train our network.\n", 
      "\n", 
      "CIFAR-10 images are 32x32, and up until this point we have used the entire image as input to our convnets. Now we will do something different: our convnet will expect a smaller input (say 28x28). Instead of feeding our training images directly to the convnet, at training time we will randomly crop each training image to 28x28, randomly flip half of the training images horizontally, and randomly adjust the contrast and tint of each training image.\n", 
      "\n", 
      "Open the file `cs231n/data_augmentation.py` and implement the `random_flips`, `random_crops`, `random_contrast`, and `random_tint` functions. In the same file we have implemented the `fixed_crops` function to get you started. When you are done you can run the cell below to visualize the effects of each type of data augmentation."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "from cs231n.data_augmentation import *\n", 
      "\n", 
      "X = get_CIFAR10_data(num_training=100, normalize=False)[0]\n", 
      "num_imgs = 8\n", 
      "print X.dtype\n", 
      "X = X[np.random.randint(100, size=num_imgs)]\n", 
      "\n", 
      "X_flip = random_flips(X)\n", 
      "X_rand_crop = random_crops(X, (28, 28))\n", 
      "\n", 
      "# To give more dramatic visualizations we use large scales for random contrast\n", 
      "# and tint adjustment.\n", 
      "X_contrast = random_contrast(X, scale=(0.5, 1.0))\n", 
      "X_tint = random_tint(X, scale=(-50, 50))\n", 
      "\n", 
      "next_plt = 1\n", 
      "for i in xrange(num_imgs):\n", 
      "    titles = ['original', 'flip', 'rand crop', 'contrast', 'tint']\n", 
      "    for j, XX in enumerate([X, X_flip, X_rand_crop, X_contrast, X_tint]):\n", 
      "        plt.subplot(num_imgs, 5, next_plt)\n", 
      "        img = XX[i].transpose(1, 2, 0)\n", 
      "        if j == 4:\n", 
      "            # For visualization purposes we rescale the pixel values of the\n", 
      "            # tinted images\n", 
      "            low, high = np.min(img), np.max(img)\n", 
      "            img = 255 * (img - low) / (high - low)\n", 
      "        plt.imshow(img.astype('uint8'))\n", 
      "        if i == 0:\n", 
      "            plt.title(titles[j])\n", 
      "        plt.gca().axis('off')\n", 
      "        next_plt += 1\n", 
      "plt.show()"
     ], 
     "metadata": {}
    }, 
    {
     "source": [
      "# Train again\n", 
      "We will now train a new network with the same training data and the same architecture, but using data augmentation and dropout.\n", 
      "\n", 
      "If everything works, you should see a higher validation accuracy than above and a smaller gap between the training accuracy and the validation accuracy.\n", 
      "\n", 
      "Networks with dropout usually take a bit longer to train, so we will use more training epochs this time."
     ], 
     "cell_type": "markdown", 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "input_shape = (3, 28, 28)\n", 
      "\n", 
      "def augment_fn(X):\n", 
      "    out = random_flips(random_crops(X, input_shape[1:]))\n", 
      "    out = random_tint(random_contrast(out))\n", 
      "    return out\n", 
      "\n", 
      "def predict_fn(X):\n", 
      "    return fixed_crops(X, input_shape[1:], 'center')\n", 
      "    \n", 
      "model = init_three_layer_convnet(filter_size=5, input_shape=input_shape, num_filters=(32, 128))\n", 
      "trainer = ClassifierTrainer()\n", 
      "\n", 
      "best_model, loss_history, train_acc_history, val_acc_history = trainer.train(\n", 
      "          X_train, y_train, X_val, y_val, model, three_layer_convnet,\n", 
      "          reg=0.05, learning_rate=0.00005, learning_rate_decay=1.0,\n", 
      "          batch_size=50, num_epochs=30, update='rmsprop', verbose=True, dropout=0.6,\n", 
      "          augment_fn=augment_fn, predict_fn=predict_fn)"
     ], 
     "metadata": {}
    }, 
    {
     "cell_type": "code", 
     "language": "python", 
     "outputs": [], 
     "collapsed": false, 
     "input": [
      "# Visualize the loss and accuracy for our network trained with dropout and data augmentation.\n", 
      "# You should see less overfitting, and you may also see slightly better performance on the\n", 
      "# validation set.\n", 
      "\n", 
      "plt.subplot(2, 1, 1)\n", 
      "plt.plot(train_acc_history)\n", 
      "plt.plot(val_acc_history)\n", 
      "plt.title('accuracy vs time')\n", 
      "plt.legend(['train', 'val'], loc=4)\n", 
      "plt.xlabel('epoch')\n", 
      "plt.ylabel('classification accuracy')\n", 
      "\n", 
      "plt.subplot(2, 1, 2)\n", 
      "plt.plot(loss_history)\n", 
      "plt.title('loss vs time')\n", 
      "plt.xlabel('iteration')\n", 
      "plt.ylabel('loss')\n", 
      "plt.show()"
     ], 
     "metadata": {}
    }
   ], 
   "metadata": {}
  }
 ], 
 "metadata": {
  "name": "", 
  "signature": "sha256:bd9b6754c8e8fbf29e73377ae44f3019ae989ce1938884188c6e163ade93794e"
 }
}